{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit card fraud detector using Fraud Detector service model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate and process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by downloading and reading in the credit card fraud data set.\n",
    "The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget https://fraud-detector-blog-assets.s3.amazonaws.com/creditcard.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "data = pd.read_csv('creditcard.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at our data (we only show a subset of the columns in the table):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class column corresponds to whether or not a transaction is fradulent. We see that the majority of data is non-fraudulent with only $492$ ($.173\\%$), check the Class column mean, of the data corresponding to fraudulent examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the mean and standard deviation of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data.hist(bins=50,figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, columns 𝑉𝑖 have been normalized to have 0 mean and unit standard deviation as the result of a PCA. Now, lets change the data to be Amazon Fraud Detector compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['time', 'v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10', 'v11', 'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18', 'v19', 'v20', 'v21', 'v22', 'v23', 'v24', 'v25', 'v26', 'v27', 'v28', 'amount', 'class'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# to lowecase\n",
    "data.columns = map(str.lower, data.columns)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['time', 'va', 'vb', 'vc', 'vd', 've', 'vf', 'vg', 'vh', 'vi', 'vj', 'vk', 'vl', 'vm', 'vn', 'vo', 'vp', 'vq', 'vr', 'vs', 'vt', 'vu', 'vv', 'vw', 'vx', 'vy', 'vz', 'vaa', 'vab', 'amount', 'class'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def standardize_headers(x):\n",
    "    if any(char.isdigit() for char in x):\n",
    "        if int(x[1:]) > 26:\n",
    "            return 'va'+chr(int(x[1:])+70)\n",
    "        return 'v'+chr(int(x[1:])+96)\n",
    "    return x\n",
    "\n",
    "# mapping number to letter\n",
    "data.rename(columns=standardize_headers, inplace=True)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then change the timestamp and label column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename to the Fraud Detector name conventions \n",
    "data.rename(columns={'time':'EVENT_TIMESTAMP','class':'EVENT_LABEL'}, inplace=True)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get epoch time for the initial dataset date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "epoch = datetime.utcfromtimestamp(0)\n",
    "def unix_time_seconds(dt):\n",
    "    return (dt - epoch).total_seconds()\n",
    "\n",
    "# Lets pretend that the data is from 2 days ago and we can test at the end with todays date.\n",
    "start_dt = datetime.strptime('Aug 4 2020  12:00AM', '%b %d %Y %I:%M%p')\n",
    "start_dt = datetime.now()\n",
    "start_ep = unix_time_seconds(start_dt)\n",
    "print(start_ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate the current timestamp format (increasing seconds) to ISO 8601 standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def to_datetime(x):\n",
    "    current_ep = start_ep + x\n",
    "    current_dt = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.localtime(current_ep))\n",
    "    return current_dt\n",
    "\n",
    "# translate seconds delta to actual datetimes in ISO 8601\n",
    "data['EVENT_TIMESTAMP'] = data['EVENT_TIMESTAMP'].apply(to_datetime)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split our dataset to get some test samples. After will upload the data to S3 using boto3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(data)) < 0.95\n",
    "test = data[~msk]\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO # python3; python2: BytesIO \n",
    "import boto3\n",
    "\n",
    "bucket = 'sample-creditcard-dataset' # already created on S3\n",
    "csv_buffer = StringIO()\n",
    "data.to_csv(csv_buffer, index=False)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket, 'creditcard-fraud-detector-training.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading the data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO # python3; python2: BytesIO \n",
    "import boto3\n",
    "\n",
    "bucket = 'sample-creditcard-dataset' # already created on S3\n",
    "csv_buffer = StringIO()\n",
    "test.to_csv(csv_buffer, index=False)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket, 'creditcard-fraud-detector-test.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the datasets ready we need create the necesary entities for build and deploy the fraud detection model. This can be done within the Amazon Fraud Detector console or through the API as shown in the second jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing our model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# -- fraud detector client --\n",
    "client = boto3.client('frauddetector',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity, Detector, Model, and File Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_TYPE    = \"transaction\" #change to your entity\n",
    "EVENT_TYPE     = \"testevent1\" #change to your envent_type\n",
    "\n",
    "DETECTOR_NAME = \"frauddetector-detector\" #change to your detector\n",
    "DETECTOR_VER  = \"1\"\n",
    "\n",
    "# -- name and version of model, used to get the model column names -- \n",
    "MODEL_NAME    = \"fraud_model\"\n",
    "MODEL_VER     = \"1\"\n",
    "\n",
    "record_count = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the test dataset from training columns and defining the start datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_variables = [column for column in test.columns if column not in  ['EVENT_LABEL', 'EVENT_TIMESTAMP']]\n",
    "dateTimeObj = datetime.strptime('Sep 3 2013  12:00AM', '%b %d %Y %I:%M%p')\n",
    "#dateTimeObj = datetime.now()\n",
    "timestampStr = dateTimeObj.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "print(' '.join(model_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid \n",
    "\n",
    "# test the endpoint with a single prediction.\n",
    "eventId = uuid.uuid1()\n",
    "testrecord = test[model_variables].head(1).astype(str).to_dict(orient='records')[0]\n",
    "pred = client.get_event_prediction(detectorId=DETECTOR_NAME, \n",
    "                                       detectorVersionId=DETECTOR_VER,\n",
    "                                       eventId = str(eventId),\n",
    "                                       eventTypeName = EVENT_TYPE,\n",
    "                                       eventTimestamp = timestampStr, \n",
    "                                       entities = [{'entityType': ENTITY_TYPE, 'entityId':str(eventId.int)}],\n",
    "                                       eventVariables=  testrecord)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block will use some parallelization to run several test against the fraud detector endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask \n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "display(HTML(\"<style>.container { width:90% }</style>\"))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "@dask.delayed\n",
    "def _predict(record):\n",
    "    eventId = uuid.uuid1()\n",
    "    try:\n",
    "        pred = client.get_event_prediction(detectorId=DETECTOR_NAME, \n",
    "                                       detectorVersionId=DETECTOR_VER,\n",
    "                                       eventId = str(eventId),\n",
    "                                       eventTypeName = EVENT_TYPE,\n",
    "                                       eventTimestamp = timestampStr, \n",
    "                                       entities = [{'entityType': ENTITY_TYPE, 'entityId':str(eventId.int)}],\n",
    "                                       eventVariables=  record) \n",
    "        \n",
    "        record[\"score\"]   = pred['modelScores'][0]['scores'][\"{0}_insightscore\".format(MODEL_NAME)]\n",
    "        record[\"outcomes\"]= pred['ruleResults'][0]['outcomes']\n",
    "        return record\n",
    "    \n",
    "    except:\n",
    "        pred  = client.get_event_prediction(detectorId=DETECTOR_NAME, \n",
    "                                       detectorVersionId='1',\n",
    "                                       eventId = str(eventId),\n",
    "                                       eventTypeName = EVENT_TYPE,\n",
    "                                       eventTimestamp = timestampStr, \n",
    "                                       entities = [{'entityType': ENTITY_TYPE, 'entityId':str(eventId.int)}],\n",
    "                                       eventVariables=  record) \n",
    "        record[\"score\"]   = \"-999\"\n",
    "        record[\"outcomes\"]= \"error\"\n",
    "        return record\n",
    "\n",
    "#just testing with 100 samples, increase the record_count variable o remove the .head to test the entire test dataset\n",
    "predict_data  = test[model_variables].head(record_count).astype(str).to_dict(orient='records')\n",
    "predict_score = []\n",
    "\n",
    "i=0\n",
    "for record in predict_data:\n",
    "    clear_output(wait=True)\n",
    "    rec = dask.delayed(_predict)(record)\n",
    "    predict_score.append(rec)\n",
    "    i += 1\n",
    "    print(\"current progress: \", round((i/record_count)*100,2), \"%\" )\n",
    "    \n",
    "predict_recs = dask.compute(*predict_score)\n",
    "\n",
    "# Calculate time taken and print results\n",
    "time_taken = time.time() - start\n",
    "tps = len(predict_recs) / time_taken\n",
    "\n",
    "print ('Process took %0.2f seconds' %time_taken)\n",
    "print ('Scored %d records' %len(predict_recs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame.from_dict(predict_recs, orient='columns')\n",
    "predictions.head(record_count)\n",
    "predictions.loc[predictions['score'] >=950, 'vaa':'outcomes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the model metrics on CloudWatch and the prediction history in Fraud Detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results to a csv file\n",
    "predictions.to_csv(MODEL_NAME + \"precictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.loc[data['vaa'] == 0.14205158164005, 'vaa':'EVENT_LABEL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28515.0,1.2266432,0.10198813,-0.08707244,0.11152376,-0.28199157,-1.356027,0.46904954,-0.37172508,-0.1536717,-0.1451049,-0.14350533,0.3209641,0.1493134,0.45251527,0.7762534,0.028739315,-0.17786655,-1.0047463,0.2649526,0.019692326,-0.35510048,-1.1536633,0.109793216,0.4203179,0.19793178,0.6992181,-0.11486099,0.007582546,50.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('creditcard.csv', delimiter=',')\n",
    "\n",
    "feature_columns = df.columns[:-1]\n",
    "label_column = df.columns[-1]\n",
    "\n",
    "features = df[feature_columns].values.astype('float32')\n",
    "labels = (df[label_column].values).astype('float32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "payload = ','.join(map(str, X_train[0]))\n",
    "\n",
    "print(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '48ce4ef8-cd3d-4271-a2ef-06a7c3a80119', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '48ce4ef8-cd3d-4271-a2ef-06a7c3a80119', 'x-amzn-invoked-production-variant': 'AllTraffic', 'date': 'Tue, 4 Aug 2020 22:33:03 GMT', 'content-type': 'text/csv; charset=utf-8', 'content-length': '21'}, 'RetryAttempts': 0}, 'ContentType': 'text/csv; charset=utf-8', 'InvokedProductionVariant': 'AllTraffic', 'Body': <botocore.response.StreamingBody object at 0x7fedf40eca20>}\n",
      "classification pred_proba: 0.0000089771, prediction: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "sagemaker_endpoint_name = 'fraud-detection-endpoint'\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "response = sagemaker_runtime.invoke_endpoint(EndpointName=sagemaker_endpoint_name, ContentType='text/csv',\n",
    "                                                 Body=payload)\n",
    "print(response)\n",
    "pred_proba = json.loads(response['Body'].read().decode())\n",
    "formatted_float = \"{:.10f}\".format(pred_proba)\n",
    "prediction = 0 if pred_proba < 0.5 else 1\n",
    "# Note: XGBoost returns a float as a prediction, a linear learner would require different handling.\n",
    "print(\"classification pred_proba: {}, prediction: {}\".format(formatted_float, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finish"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
