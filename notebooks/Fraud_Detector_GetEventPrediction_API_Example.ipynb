{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detector - Minimal Prediction API Example \n",
    "#### Supervised fraud detection  \n",
    "-------\n",
    "\n",
    "## Setup\n",
    "------\n",
    "First setup your AWS credentials so that Fraud Detector can store and access training data and supporting detector artifacts.\n",
    "\n",
    "https://docs.aws.amazon.com/frauddetector/latest/ug/set-up.html\n",
    "\n",
    "To use Amazon Fraud Detector, you have to set up permissions that allow access to the Amazon Fraud Detector console and API operations. You also have to allow Amazon Fraud Detector to perform tasks on your behalf and to access resources that you own.\n",
    "\n",
    "We recommend creating an AWS Identify and Access Management (IAM) user with access restricted to Amazon Fraud Detector operations and required permissions. You can add other permissions as needed.\n",
    "\n",
    "## Plan\n",
    "------\n",
    "\n",
    "You'll need the following pieces of information to make predictions on your dataset. \n",
    "\n",
    "- ENTITY_TYPE  \n",
    "- EVENT_TYPE    \n",
    "- DETECTOR_NAME & VERSION\n",
    "\n",
    "\n",
    "You'll also need to identify how many records you'd like to predict on.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "display(HTML(\"<style>.container { width:90% }</style>\"))\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import uuid \n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# -- dask for parallelism -- \n",
    "import dask \n",
    "\n",
    "# -- standard stuff -- \n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# -- AWS stuff -- \n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize AWS Fraud Detector Client \n",
    "------\n",
    "\n",
    "https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/frauddetector.html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- fraud detector client --\n",
    "client = boto3.client('frauddetector',)\n",
    "\n",
    "# -- use this to append to files \n",
    "sufx   = datetime.now().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity, Detector, Model, and File Information  \n",
    "-----\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Entity, Detector, and Files. </strong>\n",
    "\n",
    "- DETECTOR_NAME & VERSION corresponds to the name and version of your deployed Fraud Detector  \n",
    "- MODEL_NAME & VERSION corresponds to the name and version of the model deployed with your Fraud Detector   \n",
    "- S3_BUCKET & S3_FILE this is the information on the S3 file you wish to apply your detector to.   \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_TYPE    = \"your_entity\"\n",
    "EVENT_TYPE     = \"your_envent_type\" \n",
    "\n",
    "DETECTOR_NAME = \"your_detector\"\n",
    "DETECTOR_VER  = \"1\"\n",
    "\n",
    "# -- name and version of model, used to get the model column names -- \n",
    "MODEL_NAME    = \"your_model\"\n",
    "MODEL_VER     = \"1\"\n",
    "\n",
    "\n",
    "# -- input file of data to be scored -- \n",
    "ARN_ROLE      = \"arn:aws:iam::XXXX:role/your_role\" \n",
    "S3_BUCKET     = \"yourbucket\"\n",
    "S3_FILE       = \"yourfile.csv\"\n",
    "S3_FILE_LOC   = \"s3://{0}/{1}\".format(S3_BUCKET,S3_FILE)\n",
    "\n",
    "# -- run 100 records, you can change this here or below to run the whole file.\n",
    "record_count = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data to be Scored \n",
    "-----\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Check the first 5 Records. </strong>\n",
    "\n",
    "Does your data look correct? Do you need to rename any columns? You want the column names to match the field names used by the Model. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- connect to S3, snag file, and convert to a panda's dataframe --\n",
    "s3   = boto3.resource('s3')\n",
    "obj  = s3.Object(S3_BUCKET, S3_FILE)\n",
    "body = obj.get()['Body']\n",
    "df   = pd.read_csv(body)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Predictions  \n",
    "-----\n",
    "The following applies the get_event_prediction endpoint to your recrods in your data frame.    \n",
    "\n",
    "<i> Note: this uses the Dask backend to parallelize the prediction calls. </i>\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> get_event_prediction </strong>\n",
    "\n",
    "To specify the number of records to score you change the record_count to a specific number (e.g., if you want to just predict on say 100 records). By default it assumes you want to apply predictions to the whole dataset. Once completed convert json to a pandas dataframe, and appends any existing labels, and analyze based on score threshold for a particular false positive rate (FPR).\n",
    "\n",
    "</div>\n",
    "\n",
    "this is all you need to run predictions: \n",
    "\n",
    "```python\n",
    "\n",
    "client.get_event_prediction(detectorId=DETECTOR_NAME, \n",
    "                            detectorVersionId=DETECTOR_VERSION,\n",
    "                            eventId = '222222',\n",
    "                            eventTypeName = EVENT_TYPE,\n",
    "                            eventTimestamp = '2020-07-27 12:01:01', \n",
    "                            entities = [{'entityType': ENTITY_TYPE, 'entityId':'11111'}],\n",
    "                            eventVariables=  record)\n",
    "```\n",
    "\n",
    "\n",
    "Example of what a record would look like: \n",
    "\n",
    "```python\n",
    "record = [{'order_amt': '8036.0',\n",
    "  'ip_address': '192.18.59.93',\n",
    "  'email_address': 'synth_george_hayduke@example.com',\n",
    "  'cc_bin': '42785',\n",
    "  'billing_postal': '17740-2745',\n",
    "  'shipping_postal': '20950-6945',\n",
    "  'customer_name': 'Geroge Hayduke'}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record_count = df.shape[0] -- override  to run all records in file \n",
    "model_variables = [column for column in df.columns if column not in  ['EVENT_LABEL', 'EVENT_TIMESTAMP']]\n",
    "dateTimeObj = datetime.now()\n",
    "timestampStr = dateTimeObj.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "@dask.delayed\n",
    "def _predict(record):\n",
    "    eventId = uuid.uuid1()\n",
    "    try:\n",
    "        pred = client.get_event_prediction(detectorId=DETECTOR_NAME, \n",
    "                                       detectorVersionId=DETECTOR_VERSION,\n",
    "                                       eventId = str(eventId),\n",
    "                                       eventTypeName = EVENT_TYPE,\n",
    "                                       eventTimestamp = timestampStr, \n",
    "                                       entities = [{'entityType': ENTITY_TYPE, 'entityId':str(eventId.int)}],\n",
    "                                       eventVariables=  record) \n",
    "        \n",
    "        record[\"score\"]   = pred['modelScores'][0]['scores'][\"{0}_insightscore\".format(MODEL_NAME)]\n",
    "        record[\"outcomes\"]= pred['ruleResults'][0]['outcomes']\n",
    "        return record\n",
    "    \n",
    "    except:\n",
    "        pred  = client.get_event_prediction(detectorId=DETECTOR_NAME, \n",
    "                                       detectorVersionId='1',\n",
    "                                       eventId = str(eventId),\n",
    "                                       eventTypeName = EVENT_TYPE,\n",
    "                                       eventTimestamp = timestampStr, \n",
    "                                       entities = [{'entityType': ENTITY_TYPE, 'entityId':str(eventId.int)}],\n",
    "                                       eventVariables=  record) \n",
    "        record[\"score\"]   = \"-999\"\n",
    "        record[\"outcomes\"]= \"error\"\n",
    "        return record\n",
    "\n",
    "    \n",
    "\n",
    "#predict_data  = df[eventVariables].head(10).astype(str).to_dict(orient='records')\n",
    "predict_data  = df[model_variables].head(record_count).astype(str).to_dict(orient='records')\n",
    "predict_score = []\n",
    "\n",
    "i=0\n",
    "for record in predict_data:\n",
    "    clear_output(wait=True)\n",
    "    rec = dask.delayed(_predict)(record)\n",
    "    predict_score.append(rec)\n",
    "    i += 1\n",
    "    print(\"current progress: \", round((i/record_count)*100,2), \"%\" )\n",
    "    \n",
    "\n",
    "predict_recs = dask.compute(*predict_score)\n",
    "\n",
    "# Calculate time taken and print results\n",
    "time_taken = time.time() - start\n",
    "tps = len(predict_recs) / time_taken\n",
    "\n",
    "print ('Process took %0.2f seconds' %time_taken)\n",
    "print ('Scored %d records' %len(predict_recs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at your predictions\n",
    "-----\n",
    "Each record will have a score and the outcome of any rule conditions met. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame.from_dict(predict_recs, orient='columns')\n",
    "head(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally Write Predictions to File\n",
    "\n",
    "<div class=\"alert alert-info\"> <strong> Write Predictions. </strong>\n",
    "\n",
    "You can write your prediction dataset to a CSV to manually review predictions. Simply add a cell below and copy the code below. \n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# -- optionally write predictions to a CSV file -- \n",
    "predictions.to_csv(FILE + \".csv\", index=False)\n",
    "# -- or to a XLS file \n",
    "predictions.to_excel(FILE + \".xlsx\", index=False)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
